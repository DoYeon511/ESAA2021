{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESAA 0927 데이콘 필사.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmfTy9LZ18CG",
        "outputId": "2b7b9e2a-d5a9-4f1b-c831-32b43989721e"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "default_path='/content/gdrive/MyDrive/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-ltiVGI2PIY"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(default_path+\"train.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdsEQnla2bvE",
        "outputId": "27a1982b-546d-4d9e-fe87-65f54d0ff4f4"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  42331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUiGDg3z2gKi"
      },
      "source": [
        "# #4. word embedding\n",
        "## 1.Keras Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx7UlbytsSHv"
      },
      "source": [
        "max_len = 100\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, input_length = max_len))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6khJTLXw2p__"
      },
      "source": [
        "## 2. word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jXUs79Q2s1H"
      },
      "source": [
        "import gensim\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format(default_path+'GoogleNews-vectors-negative300.bin.gz', binary = True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7IonaCt2ze0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# ocabulary에 있는 토큰들의 벡터를 가져와 embedding matrix에 저장\n",
        "embedding_matrix = np.zeros((vocab_size, 300)) # 300차원의 임베딩 메트릭스 생성\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "for index, word in enumerate(tokenizer.word_index):  #vocabulary에 있는 토큰들을 하나씩 남겨줍니다.\n",
        "    if word in word2vec:  # 넘겨받은 토큰이 word2vec에 존재하면(이미 훈련된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word]  # 해당 토큰에 해당하는 vector을 불러오고\n",
        "        embedding_matrix[i] = embedding_vector  # 해당 위치의 embedding_matrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3cHZzHh21m2"
      },
      "source": [
        "# keras embedding layer에 embedding_matrix를 가중치로 주어 이용\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2QCHHm23pX"
      },
      "source": [
        "## 3. glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEV7Q0IctWAb"
      },
      "source": [
        "# 2. load the whole embedding into memory\n",
        "glove = dict()\n",
        "f = open(default_path+'glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype = 'float32')\n",
        "    glove[word] = vector\n",
        "f.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwgdYzD42G-i"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_matrix[i] = embedding_vector #해당 위치의 embedding_matrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"glove 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTiu-KLv3HIN"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugEtJxS-3JQL"
      },
      "source": [
        "## 4. Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuj9UtzK3K-K"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "FastText = gensim.models.KeyedVectors.load_word2vec_format(default_path+'word-embeddings/fasttext/fasttext.vec', binary=True, unicode_errors='ignore')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgJuBMhT5UO0"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_matrix[i] = embedding_vector #해당 위치의 embedding_matrix에 저장합니다."
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRrwAx8R7P2q"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = max_len))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNwjGM-u7rAD"
      },
      "source": [
        "# #5. Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFSxnh5r7v-p"
      },
      "source": [
        "## 간단한 전처리 + 형태소 분석"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8THY_ie-tIY",
        "outputId": "6dfbff2a-d455-44d1-bc9e-ea9c0965d35f"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3V2pSFyBuJV"
      },
      "source": [
        "train = pd.read_csv(default_path+\"train.csv\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVfCf8Ia7viX",
        "outputId": "dc53e2c3-c848-46d9-ec97-607b113abc1f"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import tqdm \n",
        "\n",
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] \n",
        "    tokenizer = Okt() \n",
        "    token_list = []\n",
        "    \n",
        "    for text in tqdm.tqdm(text_list):\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) \n",
        "        token = tokenizer.morphs(txt) \n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] \n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n",
        "\n",
        "train['token'], okt = text_preprocessing(train['text'])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54879/54879 [02:04<00:00, 439.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSa6qH3X_Dl5"
      },
      "source": [
        "## vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHC-c5I872nj",
        "outputId": "14297b04-ef6d-4d56-db47-ac0f074c873f"
      },
      "source": [
        "def text2sequence(train_text, max_len=1000):\n",
        "    \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_y = train['author']\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
        "print(train_X.shape, train_y.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  36342\n",
            "(54879, 100) (54879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlzOPsXi_KIN"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otcO5c9P85ll"
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format(default_path+'GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "for index, word in enumerate(tokenizer.word_index):\n",
        "    if word in word2vec:\n",
        "        embedding_vector = word2vec[word] \n",
        "        embedding_mxtrix[i] = embedding_vector \n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtZT20G1_R2h"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1V_gzY-_Um4"
      },
      "source": [
        "def LSTM(vocab_size, max_len=1000):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Txu6M8uPFDhY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}